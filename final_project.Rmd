---
title: "Final Project"
author: "Lara Dular and Indgrid de Waal"
date: "December 9, 2016"
output: html_document
---
#Introduction

<font color="red">
Insert sections from the ready-made file for suggesting the final project. Explain the variables in detail
</font>

#1. Word2Vec

<font color="red">
**Vlad's comment:**  very nice explanation! additionally , you could discuss about negative sampling and hierarchical softmax

My explanatio above already exceeds the limiation 600 words. Not by much but it does. I'll wirte him an email asking (1) if it's ok to use what's already writen (2) should we exceed the limitation and explain further or leave it as is.
</font>

The goal of Word2Vec is to mine text files (corpus) into a mathematical model that is understandable to a computer, while still preserving, or better yet, unraveling the context and relation between words. The mathematical model behind theWord2Vec is a high dimensional vector space ($\mathbb{R}^N$) where words are represented by $N$-dimensional vectors. The distances between words that share the common context should be small. The model is very good at answering analogy questions of the form: “Paris” is to “France” as “Amsterdam” is to ?.

Word2Vec is a shallow neural net with a single hidden layer. There are two ways of training the word2vec neural net, by choosing one of two models: The Continuous-Bag-of-Words model (CBOW) or the Skip-Gram model (SG). The models are very similar and are basically mirror images of one another -- CBOW predicts a target word (output) from source context words (input layer), while the SG does the inverse and predicts context words (output layer) of the input word (input layer). The output of both models is the projection matrix, which is a by-product of the neural network. 

The structure of the neural net differs based on the model chosen. Let’s first focus on the SG model. Let $V$ denote the number of words in vocabulary. The vocabulary is built automatically from unique words in the corpus. To control the size of vocabulary it is useful to set the minimum frequency of words occurrences. The input layer has as many neurons as there are words in the vocabulary.  As mentioned, the SG model's input is a single word, meaning that the input layer is actually a one-hot vector $\mathbf{x}$ of length $V$. To get from the input to the hidden layer we multiply $\mathbf{x}$ by a $V\times N$ dimensional weight matrix $\mathbf{W}$, where the $i$-th row of matrix $\mathbf{W}$ is an $N$-dimensional representation of word $w_i$, $i = 1,\ldots,V$. 
Formally, $\mathbf{W}_{(i,\cdot)} = \mathbf{v}_{w_i}^\intercal$. Since $\mathbf{x}$ is a one-hot vector, we can assume, without loss of generality, that $\mathbf{x}$ is the $i$-th word in the vocabulary -- $x_i=1$ and $x_k=0,\ k\neq i$. Then we can calculate the hidden layer vector $\mathbf{h}$ as 
$$\mathbf{h} = \mathbf{W}^\intercal\mathbf{x} = {\mathbf{v}_{w_i}}^\intercal$$
meaning that the hidden layer corresponds to the $i$-th row of $\mathbf{W}$. This also implies that the hidden layer activation function is linear. The hidden layer is $N$-dimensional representation (word vector) of word $w_i$. 

Now we need to get from the hidden layer to the output layer. To do so, we first multiply the $N$ dimensional vector $\mathbf{h}$ by a $N \times V$ dimensional weight matrix $\mathbf{W}'$. Similarly as before, the $j$-th column of matrix $\mathbf{W}'$ is an $N$-dimensional representation of word $w_j$ -- formally, $\mathbf{W}'_{(\cdot,j)} = \mathbf{v}'_{w_j}$. Since we wish to have an output compounded out of $C$ context words, we have to give as an output $C$ multinomial distributions of length $V$ -- one for each of the context words. To make the calculations more clear, we denote $C=2n$, so the context of the word $w_i$ are words $w_{-n+i},w_{i-n+1},\ldots w_{i-1},w_{i+1},\ldots w_{i+n-1}+w_{i+n}$. The $C$ score vectors $u_{w_{i-n}},\ldots u_{w_{i-1}},u_{w_{i+1}},\ldots u_{w_{i+n}}$ are all replicas of vector

$$
u = \mathbf{W' h},
$$ 
where the $j-th$ component equals 
$$u_j = {\mathbf{v'}_{w_j}}^\intercal \mathbf{h} = {\mathbf{v'}_{w_j}}^\intercal {\mathbf{v}_{w_i}}^\intercal.$$ 

Since the goal is produce probabilities that context words $w_{-n+i},w_{i-n+1},\ldots w_{i-1},w_{i+1},\ldots w_{i+n-1}+w_{i+n}$ appear in the context of $w_i$, we transform each of the scores using softmax activation function (or normalized exponential function) in the following way

$$
p(w_{i+j}|w_i) = \frac{\exp({\mathbf{v'}_{w_{i+j}}}^\intercal\mathbf{h}) }{\sum_{j'=1}^V \exp({\mathbf{v'}_{w_{j'}}}^\intercal\mathbf{h})},\ j\in \{-n,\ldots,-1,1,\ldots n\}.
$$

At this point the calculations need to be evaluated, so an apropriate objective function need to be constructed. In this case we wish to minimize the loss function

$$
E= - \ln p(w_{i-n},\ldots w_{i-1},w_{i+1}\ldots w_{i+n}|w_i)\\
= -\ln \prod_{j=-n,\ j\neq 0}^n p(w_{i+j}|w_i)\\
= -\sum_{j=-n,\ j\neq 0}^n {\mathbf{v'}_{w_{i+j}}}^\intercal\mathbf{h} +2n\ln\sum_{j'=1}^V \exp({\mathbf{v'}_{w_{j'}}}^\intercal\mathbf{h})  
$$

By calculating the derivatives of E w.r.t. the unknown parameters we obtain the prediction errors. The prediction errors are then summed up over all context words. The weights are then adjusted accordingly trough backpropagation. These steps are run multiple times before we get the output.

In the initialization step (before the training begins) matrices $\mathbf{W}$ and $\mathbf{W'}$ are initialized to small random values.

Now we turn our attention to the CBOW model. Let $C$ denote the number of context words and $V$ represent the number of words in vocabulary. Each of the $C$ words is represented as a one-hot vector $\mathbf{x}_{k},\ k=1,\ldots C,$ of length $V$. As an input we therefore have $C$ one-hot vectors. We multiply each of the $\mathbf{x}_1,\ldots, \mathbf{x_C}$ with an $V \times N$ dimensional weight matrix $\mathbf{W}$, where the $i$-th row of the matrix represents $N$-dimensional representation of word $w_i$, $i = 1,\ldots,V$ -- formally, $\mathbf{W}_{i,\cdot} = \mathbf{v}_{w_i}^\intercal$. In the next step an average of the vectors is taken in order to get the $N$ dimensional hidden layer $\mathbf{h}$
$$\mathbf{h}=\frac{1}{C} W^\intercal (\mathbf{x}_1+\ldots+\mathbf{x}_C).$$ 

This again implies that the hidden layer activation function is linear. The following steps are simmilar to those in SG model with the destinction that one vector is produced using the stepmax activation function 

$$
p(w_i|w_1,\ldots w_C) = \frac{\exp({\mathbf{v'}_{w_j}}^\intercal \mathbf{h})}{\sum_{j'=1}^V \exp({\mathbf{v'}_{w_{j'}}}^\intercal \mathbf{h})},
$$

where $\mathbf{W}'_{(\cdot,j)} = \mathbf{v}'_{w_j}$.



#2. LDA


#3. Experiments
##3.1. Acquiring data with web scraping
```{r, echo=FALSE, message=FALSE}
setwd("C:/Users/Lara/Documents/Faks201617/MachineLearn/Assignments/Final/Rep") 
```


Since the data described above didn't include any text data, we scraped IMDB page to get summaries of the movie plots in our data set. Since our data set includes the weblink of each movie, the scraping was fairly easy. With the use of regular expressions, the ID of the movie was taken form the link as to create a link to the summary page. Then, summary of the plot can be aquired usint the appropriate xpath. If there was no summary to scrape, the movie was removed from the database. We were left with 4689 cases.

```{r, message=FALSE}
#import data
dataTrain <- read.csv("movie_metadata.csv", stringsAsFactors = FALSE)
library(stringr) #regular expr.
library(rvest)   #html extraction
library(pryr)
```
```{r, eval=F}
#### SCRAPING SUMMARY ####
summary <- sapply(X=dataTrain$movie_imdb_link, function(weblink){
  #Extract the title and number
  title <- str_extract(pattern = "/title/tt[0-9]*/",string=weblink)
  weblinkPlot <- paste0("https://www.imdb.com",title,"plotsummary")
  rawpage <- read_html(weblinkPlot)

  summaryNode <- html_nodes(x=rawpage, xpath ="(//ul[@class = 'zebraList']/li[@class='odd']/p[@class='plotSummary'])[1]")
  return(html_text(summaryNode))
})

summaryVec <- sapply(summary,FUN = function(t){if(length(t)==0){NA}else{t}})
sum(sapply(summaryVec,is.na))
rm(summary)
dataTrain$summary <- summaryVec
dataTrain <- dataTrain[!sapply(summaryVec,is.na),]


#write on disc
write.csv(x=dataTrain, file="movie_metadata_summary.csv", row.names = FALSE)
```

##3.2. Preprocessing
Once the data was collected we had to preprocess it. The summaries were collected into a corpora. With the use of package ```tm``` punctuations, numbers, stop words and additional white space were removed. We considered if the documents should be stemmed or not and have decided to do so as to make the data denser. Finally, we can write each preprocessed document on disk. 

```{r, eval=FALSE}
library(tm) #For text mining
library(SnowballC) #For text mining

#Import data 
dataAll <- read.csv("movie_metadata_summary.csv")
#Remove the annoying letter form the titles (not necesairy but its visualy nicer :) )
dataAll$movie_title <- gsub(pattern = "Â",replacement = "",x = dataAll$movie_title, fixed = T)


#Create a corpus of the summaries and preprocess  
corpusSum <- VCorpus(VectorSource(dataAll$summary) ,readerControl = list(language = "en", reader = readPlain))
as.character(corpusSum[[1]])  #print the first summary

###PREPROCESS###
#remove punctuations
corpusSum <- tm_map(corpusSum, removePunctuation)
#and numbers
corpusSum <- tm_map(corpusSum, removeNumbers)
#and transform the comments to lowercase (we don't want the destinction between lower and upper case strings)
corpusSum <- tm_map(corpusSum, tolower)
#remove english stopwords without analytic value e.g. and, such, as, so,...
corpusSum <- tm_map(corpusSum, removeWords, stopwords("english"))
#Strip additional spaces that might occur and seqeunces such as new line "\n"" 
corpusSum <- tm_map(corpusSum, stripWhitespace)
#stemming makes data denser, thus reducing the amount of data required for adequate training
corpusSum <- tm_map(corpusSum, stemDocument)
#Converting reviews it in to plain text
corpusSum <- tm_map(corpusSum, PlainTextDocument)


 
#Write the files on disk
# 
library(stringr) #for the line bellow
ids <- str_pad(1:length(corpusSum), 4, pad = "0")
writeCorpus(x = corpusSum
            ,path = "C:/Users/Lara/Documents/Faks201617/MachineLearn/Assignments/Final/Rep/data"
            ,filenames = sapply(ids, function(i) paste0(i,".txt"), USE.NAMES = F))

```




<font color="red">
**IDEA 1:**  

- First construct a model that perdicts IMDB rate without discription
- Second construct a model that will include description
- Compare the two models. What is the added value of adding a description? Maybe even construct just a model with the description
</font>


<font color="red">
**IDEA 2:**
Add a new categorical variable, one of to options is possible:

- the gross is more than the budget doubled
- the gross is more than the budget i.e. there was no loss

Check if we can predict this variable using the plot summaries.
</font>

#Results including a discussion comparing the results of LDA and Word2Vec.

#Conclusions.


##Contribution of each member.